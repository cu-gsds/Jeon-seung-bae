# data
```{r}
options(digits = 10)
# df = list()
# for (i in 1:30) {
#   if(i < 10)  df[[i]] = read.csv(paste0("data/201609", "0", i, ".csv"))  
#   else  df[[i]] = read.csv(paste0("data/201609", i, ".csv"))  
# }
# 
# library(rlist)
# df = list.rbind(df)
# 
# n = unique(df$LINK)
# df = df[df$LINK == n[1],]
# write.csv(df, "data/example.csv")
df = read.csv("data/example.csv")
```
# training length
```{r}
df <- as.matrix(df$vel)
train_length = round(length(df)*0.7) # no related time steps
test_length = length(df) - round(length(df)*0.7)
```
# scale
```{r}
train_data <- df[1:train_length,]
df = (df-min(train_data))/(max(train_data)-min(train_data))
```
# generator
```{r}
generator <- function(df, lookback, delay, min_index, max_index, shuffle = F, batch_size = batch_size, step = step){
  if (is.null(max_index)) max_index <- nrow(df) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), lookback / step, dim(df)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1, length.out = dim(samples)[[2]])
      samples[j,,] <- df[indices,]
      targets[[j]] <- df[rows[[j]] + delay, 1]
    }            
    
    list(samples, targets)
  }
}


lookback <- 114 # time step => ACF
step <- 1
delay <- 0
batch_size <- 114*3 # df%batch == 0


train_gen <- generator(
  df,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = train_length,
  shuffle = F,
  step = step,
  batch_size = batch_size
)

val_gen <- generator(
  df,
  lookback = lookback,
  delay = delay,
  min_index = train_length+1,
  max_index = length(df),
  step = step,
  batch_size = batch_size
)

# a = train_gen()
```


```{r}
library(dplyr)
library(keras)
install_keras()
# batch_input_shape = batch_size, time_steps, features
# input_shape = time_steps, features

model <- keras_model_sequential() %>% 
  layer_lstm(units = dim(df)[[-1]], batch_input_shape = c(batch_size, lookback, dim(df)[[-1]]), stateful = T, input_shape = c(lookback, dim(df)[[-1]])) %>%
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae",
  metric = "mse"
)

summary(model)
```


```{r}
# Traditionally, the steps per epoch is calculated as train_length/batchSize 2394/342

history1 <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 700,
  epochs = 5,
  validation_data = val_gen,
  validation_steps = 3
)
```


```{r}
#generator_test
generator <- function(df, lookback, delay, min_index, max_index, shuffle = F, batch_size = batch_size, step = step){
  if (is.null(max_index)) max_index <- nrow(df) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(df)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1, length.out = dim(samples)[[2]])
      samples[j,,] <- df[indices,]
      targets[[j]] <- df[rows[[j]] + delay, 1]
    }            
    
    list(samples)
  }
}


test_gen <- generator(
  df,
  lookback = lookback,
  delay = delay,
  min_index = train_length + 1,
  max_index = length(df),
  step = step,
  batch_size = batch_size
)
```


```{r}
preds <- model %>% predict_generator(test_gen, steps = 1) 

preds = (preds*(max(train_data)-min(train_data))) + min(train_data)
write.csv(preds, paste0("preds_", l, ".csv"))
df03$time = as.POSIXct(df03$time)
df03$pre = df03$vel
df03$pre[2851:3420] = preds
df03$key = 1
df03$key[2737:3420] = 2
df03$key2 = 1
df03$key2[2851:3420] = 3
library(plotly)
library(ggplot2)
  p = list()
  p[[l]] = df03 %>%
    ggplot(aes(time)) +
    geom_point(aes(y = vel), color = df03$key) +
    geom_point(aes(y = pre), color = df03$key2) +
    # geom_line(aes(y = pre), color = df03$key2, size = 1) +
    # geom_line(aes(y = vel), color = df03$key, alpha = 0.5) +
    
    labs(
        title = "DTG DATA"
    )
ggplotly(p[[l]])
ggsave(paste0("preds_", n[l], ".png"))


library(Metrics)
a = na.omit(df03[df03$key2 == 3,])
lstmMAE = list()
lstmMAE[[l]] = mae(a$vel, a$pre)
MAE <- data.frame("mae" = c(8, lstmMAE[[l]]), "model" = c("ARIMA", "LSTM"))
ggplot(MAE, aes(model, mae, color = model)) + geom_bar(stat = "identity", fill = "white")
ggsave(paste0("MAE_", n[l], ".png"))

save_model_hdf5(model, paste0("model_", n[l]))
write.csv(lstmMAE[[l]], paste0("lstmMAE_", n[l], ".csv"))
}
```
